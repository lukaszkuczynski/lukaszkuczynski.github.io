{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nltk_sources.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "m0K8zWzF9pmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## NLTK revisited: why\n",
        "When you start working with some text-analysis project, sooner or later you will encounter the following problem: Where to find sample text, how to get resources, where should I start.  When I [first had a contact (Polish language post)](http://lukcreates.pl/dajsiepoznac2017/porownanie-tekstu-scikit-learn-i-nltk-nlp/) with NLP I didn't appreciate the power that lies behind the NLTK - the Python first-choice library for NLP.  However, after several years, I see that I could use it earlier.  Why?  NLTK comes with an easy access to various sources of text. And I am going to show you what particularly I like and what caught my attention when studying the first 3 chapters of an [official book](http://nltk.org/book/).\n",
        "\n",
        "But the main business is what?  I would like to (finally) build some *Suggestion tool* that will allow providing some Virtual Assistant to help in decision making progress."
      ]
    },
    {
      "metadata": {
        "id": "BqXJBNS79mXu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bundled resources available\n",
        "### brown and its categories\n",
        "NLTK comes with various corpora, so big packs of text.  You can utilize them as shown in the following example.  All you need to do is to download appropriate corpus and start exploring that.  Let us see now."
      ]
    },
    {
      "metadata": {
        "id": "xDcz6_1vIGru",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "x0zOn10oZyY-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a7ea7e2b-f6fd-4ad9-a724-dfce819f3d60"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "files = nltk.corpus.brown.fileids()\n",
        "print(f\"'Brown' corpus contain {len(files)} files\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "'Brown' corpus contain 500 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cwGVUcwpD3oO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this corpus you will find different text categorized into categories.  So it nicely fits into classification area of machine learning.  Following there are categories of these texts together with some samples."
      ]
    },
    {
      "metadata": {
        "id": "HUEbLMbREJZs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "6c4b3fbb-008e-40e8-f3ac-fdc7b19d0556"
      },
      "cell_type": "code",
      "source": [
        "print(\"'brown' contains following categories %s\" % nltk.corpus.brown.categories())\n",
        "brown_adventure = nltk.corpus.brown.sents(categories='adventure')[0:5]\n",
        "brown_government = nltk.corpus.brown.sents(categories='government')[0:5]\n",
        "print(\"Following we have some sentences from 'adventure' category:\")\n",
        "for sent in brown_adventure:\n",
        "  print(\" > \"+ \" \".join(sent))\n",
        "print(\"And here we have some sentences from 'government' category:\")\n",
        "for sent in brown_government:\n",
        "  print(\" > \"+ \" \".join(sent)) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'brown' contains following categories ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
            "Following we have some sentences from 'adventure' category:\n",
            " > Dan Morgan told himself he would forget Ann Turner .\n",
            " > He was well rid of her .\n",
            " > He certainly didn't want a wife who was fickle as Ann .\n",
            " > If he had married her , he'd have been asking for trouble .\n",
            " > But all of this was rationalization .\n",
            "And here we have some sentences from 'government' category:\n",
            " > The Office of Business Economics ( OBE ) of the U.S. Department of Commerce provides basic measures of the national economy and current analysis of short-run changes in the economic situation and business outlook .\n",
            " > It develops and analyzes the national income , balance of international payments , and many other business indicators .\n",
            " > Such measures are essential to its job of presenting business and Government with the facts required to meet the objective of expanding business and improving the operation of the economy .\n",
            " > Contact\n",
            " > For further information contact Director , Office of Business Economics , U.S. Department of Commerce , Washington 25 , D.C. .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zpiQZv0pGJWr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There is a very important component of NLP in the *brown* corpus, namely Part Of Speech tagging (POS).  How is it organized?  Let us see the example from one of the sentences printed just minutes ago."
      ]
    },
    {
      "metadata": {
        "id": "xA8B1UNBGfOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "cellView": "both",
        "outputId": "1846054d-b1f8-4925-97ab-d6a372ea8261"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "adv_words = nltk.corpus.brown.words(categories='adventure')\n",
        "print(adv_words[:10])\n",
        "adv_words = nltk.corpus.brown.tagged_words(categories='adventure')\n",
        "print(adv_words[:10])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.']\n",
            "[('Dan', 'NP'), ('Morgan', 'NP'), ('told', 'VBD'), ('himself', 'PPL'), ('he', 'PPS'), ('would', 'MD'), ('forget', 'VB'), ('Ann', 'NP'), ('Turner', 'NP'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gLmCU-UoHRWn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Yes! It is tagged and ready to be analyzed.  What does these symbols mean?  They are symbols of parts of speech, nicely described [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).  F.e. `NP` stands fo *Proper Noun* and `VBD` is *Verb, past tense*."
      ]
    },
    {
      "metadata": {
        "id": "vRWjNOntIIzI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### sentiments\n",
        "One of the areas where NLP is used is a Sentiment Analysis, playing an important role in digital marketing.  Imagine how nice it is to process vast amount of opinions and instantly recognize whether the product is approved or rejected by a community.  Seeing trends is also possible with that.  So what is the one of the NLKT bundled tools to deal with sentiments?  This is `opinion_lexicon`"
      ]
    },
    {
      "metadata": {
        "id": "6r6IIOfPIqvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "057bab2a-841f-468f-e43e-e0c8d29838cd"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import opinion_lexicon\n",
        "nltk.download('opinion_lexicon')\n",
        "negatives = opinion_lexicon.negative()[:10]\n",
        "positives = opinion_lexicon.positive()[:10]\n",
        "print(\"If you find some negative words, here you are: %s\" % negatives)\n",
        "print(\"But let us try to see the positive side of life! Described with these words: %s\" % positives)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/opinion_lexicon.zip.\n",
            "If you find some negative words, here you are: ['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted']\n",
            "But let us try to see the positive side of life! Described with these words: ['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gOnuQjw8Jvnz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### and much more...\n",
        "There is a lot of other corpora available there.  This is not the task of this post/notebook to repeat sth what one can read him/herself in the [official papers as here](http://www.nltk.org/howto/corpus.html#tagged-corpora).  With NLTK after downloading some material, you have access to such materials as:\n",
        "* multilingual corpora (like *Universal Declaration of Human Rights*\twith 300+ languages)\n",
        "* lexical resources (*WordNet*)\n",
        "* pronouncing dictionaries (*CMU Pronouncing Dictionary*)\n",
        "\n",
        "Lots of things to browse.  But it is worth just to take a look on some of them, to have this feeling \"I saw it somewhere..\" when facing some NLP task.\n"
      ]
    },
    {
      "metadata": {
        "id": "sCKDR0io_87L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fetch anything\n",
        "If attached resources will not be enough for you, just start using different resources.\n",
        "\n",
        "### requests\n",
        "Python has libraries for anything so this is possible to use available NET resources in your app just having their URL available.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "KenktwA5GBgk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1952d5cb-1768-434b-a7d5-a00807ad396f"
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = 'https://databricks.com/blog/2018/09/26/whats-new-for-apache-spark-on-kubernetes-in-the-upcoming-apache-spark-2-4-release.html'\n",
        "resp = requests.get(url)\n",
        "blog_text = resp.text\n",
        "blog_text[:200]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\r\\n<!DOCTYPE html>\\r\\n<html lang=\"en-US\" prefix=\"og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# video: http://ogp.me/ns/video# ya: http://webmaster.yandex.ru/vocabularies/\">\\r\\n<head>\\r\\n  <meta charset=\"UTF'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "eXXgtJaCFqcu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RSS\n",
        "Ready to consume RSS?  With Python, nothing is easier.  You can create an instance of `nltk.Text` having RSS feed as the input.  This is a snippet how it could be done"
      ]
    },
    {
      "metadata": {
        "id": "56tivtWAF07k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "d1b8178b-cff7-4bc2-dbee-de1783686114"
      },
      "cell_type": "code",
      "source": [
        "!pip install feedparser\n",
        "import feedparser\n",
        "url = 'http://jvm-bloggers.com/pl/rss.xml'\n",
        "d = feedparser.parse(url)\n",
        "title = d['feed']['title']\n",
        "entries = d['entries']\n",
        "print(\"Look ma! I've just parsed RSS from a very nice Polish blogging platform. It has a title %s\" % title)\n",
        "print(\"And there we go with 5 exemplary entries:\")\n",
        "for entry in entries[:5]:\n",
        "  print(' > ' + entry.title)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K    100% |████████████████████████████████| 194kB 7.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: feedparser\n",
            "  Running setup.py bdist_wheel for feedparser ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "Successfully built feedparser\n",
            "Installing collected packages: feedparser\n",
            "Successfully installed feedparser-5.2.1\n",
            "Look ma! I've just parsed RSS from a very nice Polish blogging platform. It has a title JVMBloggers\n",
            "And there we go with 5 exemplary entries:\n",
            " > Odpowiedź: 42\n",
            " > Thanks for explaining the behaviour of dynamic (partition overwrite) mode.\n",
            " > Non-blocking and async Micronaut - quick start (part 3)\n",
            " > Strefa VIP\n",
            " > Mikroserwisy – czy to dla mnie?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-JRgx3G4HZF4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### cleaning\n",
        "When your HTML doc is fetched you probably have a doc that is full of HTML mess and there is no added value of having `<body>` in your text.  So there is some clean-up work that can be done and there are tools that can make it happen, but they are not part of nltk package.  So let us have BeautifulSoap as an example."
      ]
    },
    {
      "metadata": {
        "id": "KWTjWvmWH7QR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa581159-6d6a-482e-b2bf-6c3837a4d34e"
      },
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(blog_text)\n",
        "content = soup.find(\"div\", {'class':\"blog-content\"})\n",
        "text_without_markup = content.get_text()[:100]\n",
        "text_without_markup"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\nWhat’s New for Apache Spark on Kubernetes in the Upcoming Apache Spark 2.4 Release\\n\\n\\nSeptember 26'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "pHDvFmYyTd8i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normalization - languages are not easy\n",
        "Your language is not easy.  If you are Polish like me, it is soooo true.  But even English and other European languages add complexity to NLP.  Why?  Words have different forms and we have to conform grammar rules to be respected when analyzing text by the machine.  Taking English `going` word as an example, you mean `go` verb, but there is also its lemma `-ing` that has to be recognized and skipped for the moment of analysis.  What are 3 processes that have in-built support in NLTK?  Read the following."
      ]
    },
    {
      "metadata": {
        "id": "Ourb1HiNT5WF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "Text consists of sentences and these contain words.  Often times, we would like to have words presented as vectors as we will apply some algebra to that.  The simplest approach of tokenization can be implemented as follows, however, there are some limitations.  You can use a variety of *tokenizers* available in `nltk.tokenize` [package](https://www.nltk.org/api/nltk.tokenize.html). "
      ]
    },
    {
      "metadata": {
        "id": "iHWonu3qUe_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "cf9a52db-e9aa-488e-d909-9db7481222b7"
      },
      "cell_type": "code",
      "source": [
        "# write tokenizer yourself?\n",
        "import re\n",
        "text = \"Two smart coders are coding very quickly. Why? The end of the sprint is coming! The code has to be finished!\"\n",
        "tokens_manual = re.split(r\"[\\s+]\", text)\n",
        "print(\"Tokens taken manually %s \" % tokens_manual)\n",
        "\n",
        "\n",
        "# or maybe choose the one from the abundance in `nltk`\n",
        "import nltk\n",
        "from nltk.data              import load\n",
        "from nltk.tokenize.casual   import (TweetTokenizer, casual_tokenize)\n",
        "from nltk.tokenize.mwe      import MWETokenizer\n",
        "from nltk.tokenize.punkt    import PunktSentenceTokenizer\n",
        "from nltk.tokenize.regexp   import (RegexpTokenizer, WhitespaceTokenizer,\n",
        "                                    BlanklineTokenizer, WordPunctTokenizer,\n",
        "                                    wordpunct_tokenize, regexp_tokenize,\n",
        "                                    blankline_tokenize)\n",
        "from nltk.tokenize.repp     import ReppTokenizer\n",
        "from nltk.tokenize.sexpr    import SExprTokenizer, sexpr_tokenize\n",
        "from nltk.tokenize.simple   import (SpaceTokenizer, TabTokenizer, LineTokenizer,\n",
        "                                    line_tokenize)\n",
        "from nltk.tokenize.texttiling import TextTilingTokenizer\n",
        "from nltk.tokenize.toktok   import ToktokTokenizer\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "from nltk.tokenize.util     import string_span_tokenize, regexp_span_tokenize\n",
        "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens taken manually ['Two', 'smart', 'coders', 'are', 'coding', 'very', 'quickly.', 'Why?', 'The', 'end', 'of', 'the', 'sprint', 'is', 'coming!', 'The', 'code', 'has', 'to', 'be', 'finished!'] \n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['Two', 'smart', 'coders', 'are', 'coding', 'very', 'quickly', '.', 'Why', '?', 'The', 'end', 'of', 'the', 'sprint', 'is', 'coming', '!', 'The', 'code', 'has', 'to', 'be', 'finished', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JVz55f-fX-c9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "So one of the tasks that can be done is *stemming*, so getting rid of the words ending.  Let us see what does the popular *stemmer* does to the text we already tokenized before."
      ]
    },
    {
      "metadata": {
        "id": "tckImEnPYgYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a1c82e5b-5a44-4de3-fa2c-c1959ed3b14d"
      },
      "cell_type": "code",
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "tokens_stemmed = [porter.stem(token) for token in tokens]\n",
        "print(tokens_stemmed)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['two', 'smart', 'coder', 'are', 'code', 'veri', 'quickli', '.', 'whi', '?', 'the', 'end', 'of', 'the', 'sprint', 'is', 'come', '!', 'the', 'code', 'ha', 'to', 'be', 'finish', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PYm_aH_HZUeF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "If stemming is not enough, there has to be *lemmatization* done, so your words can be classified against the real dictionary.  Following there is an example of running this for our text sample."
      ]
    },
    {
      "metadata": {
        "id": "Z8IfdvFuazQY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "6d9863bb-72d4-409d-9f1c-30249bcf5fce"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "wnl = nltk.WordNetLemmatizer()\n",
        "lemmas = [wnl.lemmatize(token) for token in tokens]\n",
        "print(lemmas)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "['Two', 'smart', 'coder', 'are', 'coding', 'very', 'quickly', '.', 'Why', '?', 'The', 'end', 'of', 'the', 'sprint', 'is', 'coming', '!', 'The', 'code', 'ha', 'to', 'be', 'finished', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NZHWf9r-aSrO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "So where did we go?  I have just analyzed NLKT book available online, chapters no 2 and 3.  I gave a try for few tools from the big number available in this Natural Language Toolkit.  Now there is a time to explore other chapters over there.  Stay tuned.  I have to build my *Suggestion tool*."
      ]
    },
    {
      "metadata": {
        "id": "Nl3tTjrw9kzY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}